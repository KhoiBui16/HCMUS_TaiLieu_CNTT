#= Nguá»“n tham kháº£o
[1] Mykel J. Kochenderfer, Tim A. Wheeler, Kyle H. Wray. Algorithms for Decision Making, Massachusetts Institute of Technology, 2022
[2] DecisionMakingProblems.jl Repository on Github https://github.com/algorithmsbooks/DecisionMakingProblems.jl.git
=#

using Random
using Distributions
using LinearAlgebra
using Plots
# [1]
struct SetCategorical{S}
    elements::Vector{S} # Set elements (could be repeated)
    distr::Categorical # Categorical distribution over set elements
    function SetCategorical(elements::AbstractVector{S}) where S
        weights = ones(length(elements))
        return new{S}(elements, Categorical(normalize(weights, 1)))
    end
    function SetCategorical(elements::AbstractVector{S}, weights::AbstractVector{Float64}) where S
        â„“â‚ = norm(weights,1)
        if â„“â‚ < 1e-6 || isinf(â„“â‚)
            return SetCategorical(elements)
        end
        distr = Categorical(normalize(weights, 1))
        return new{S}(elements, distr)
    end
end
Distributions.rand(D::SetCategorical) = D.elements[rand(D.distr)]
Distributions.rand(D::SetCategorical, n::Int) = D.elements[rand(D.distr, n)]
function Distributions.pdf(D::SetCategorical, x)
    sum(e == x ? w : 0.0 for (e,w) in zip(D.elements, D.distr.p))
end
# [1]

# [2]
# Lá»›p SimpleGame
struct SimpleGame
    Î³  # discount factor
    â„  # agents
    ğ’œ  # joint action space
    R  # joint reward function
end

# Äá»‘i tÆ°á»£ng trÃ² chÆ¡i Rock Paper Scissors
struct RockPaperScissors end
# Sá»‘ ngÆ°á»i tham gia
n_agents(simpleGame::RockPaperScissors) = 2
# Táº­p cÃ¡c hÃ nh Ä‘á»™ng mÃ  má»—i ngÆ°á»i cÃ³ thá»ƒ lá»±a chá»n
ordered_actions(simpleGame::RockPaperScissors, i::Int) = [:rock, :paper, :scissors]
# Báº£ng cÃ¡c hÃ nh Ä‘á»™ng cÃ¡c cáº£ hai ngÆ°á»i tham gia
ordered_joint_actions(simpleGame::RockPaperScissors) = vec(collect(Iterators.product([ordered_actions(simpleGame, i) for i in 1:n_agents(simpleGame)]...)))
# Äá»™ dÃ i cá»§a ordered_actions vÃ  ordered_joint_actions
n_joint_actions(simpleGame::RockPaperScissors) = length(ordered_joint_actions(simpleGame))
n_actions(simpleGame::RockPaperScissors, i::Int) = length(ordered_actions(simpleGame, i))
# Äiá»ƒm thÆ°á»Ÿng cá»§a agent i 
function reward(simpleGame::RockPaperScissors, i::Int, a)
    if i == 1
        noti = 2
    else
        noti = 1
    end

    if a[i] == a[noti]
        r = 0.0
    elseif a[i] == :rock && a[noti] == :paper
        r = -1.0
    elseif a[i] == :rock && a[noti] == :scissors
        r = 1.0
    elseif a[i] == :paper && a[noti] == :rock
        r = 1.0
    elseif a[i] == :paper && a[noti] == :scissors
        r = -1.0
    elseif a[i] == :scissors && a[noti] == :rock
        r = -1.0
    elseif a[i] == :scissors && a[noti] == :paper
        r = 1.0
    end

    return r
end
# Báº£ng Ä‘iá»ƒm cho táº¥t cáº£ cÃ¡c trÆ°á»ng há»£p
function joint_reward(simpleGame::RockPaperScissors, a)
    return [reward(simpleGame, i, a) for i in 1:n_agents(simpleGame)]
end
# Táº¡o ra má»™t SimpleGame cho trÃ² chÆ¡i RockPaperScissors
function SimpleGame(simpleGame::RockPaperScissors)
    return SimpleGame(
        0.9,
        vec(collect(1:n_agents(simpleGame))),
        [ordered_actions(simpleGame, i) for i in 1:n_agents(simpleGame)],
        (a) -> joint_reward(simpleGame, a)
    )
end
# [2]

# [1]
# Äá»‘i tÆ°á»£ng SimpleGamePolicy chá»©a phÃ¢n phá»‘i xÃ¡c suáº¥t cá»§a cÃ¡c hÃ nh Ä‘á»™ng
struct SimpleGamePolicy
    p # dictionary mapping actions to probabilities
    function SimpleGamePolicy(p::Base.Generator)
        return SimpleGamePolicy(Dict(p))
    end
    function SimpleGamePolicy(p::Dict)
        vs = collect(values(p))
        vs ./= sum(vs)
        return new(Dict(k => v for (k,v) in zip(keys(p), vs)))
    end
    SimpleGamePolicy(ai) = new(Dict(ai => 1.0))
end

(Ï€i::SimpleGamePolicy)(ai) = get(Ï€i.p, ai, 0.0)

# Chá»n ra sá»± lá»±a chá»n, náº¿u nhiá»u lá»±a chá»n thÃ¬ random
function (Ï€i::SimpleGamePolicy)()
    D = SetCategorical(collect(keys(Ï€i.p)), collect(values(Ï€i.p)))
    println(D)
    print(rand(D))
    return rand(D)
end

joint(X) = vec(collect(Iterators.product(X...)))
joint(Ï€, Ï€i, i) = [i == j ? Ï€i : Ï€j for (j, Ï€j) in enumerate(Ï€)]

# tÃ­nh utility 
function utility(ğ’«::SimpleGame, Ï€, i)
    ğ’œ, R = ğ’«.ğ’œ, ğ’«.R
    p(a) = prod(Ï€j(aj) for (Ï€j, aj) in zip(Ï€, a))
    return sum(R(a)[i]*p(a) for a in joint(ğ’œ))
end

# tráº£ vá» sá»± lá»±a chá»n tá»‘t nháº¥t
function best_response(ğ’«::SimpleGame, Ï€, i)
    U(ai) = utility(ğ’«, joint(Ï€, SimpleGamePolicy(ai), i), i)
    ai = argmax(U, ğ’«.ğ’œ[i])
    return SimpleGamePolicy(ai)
end

# lá»›p FictitiousPlay lÃ  cÃ¡ch chÆ¡i cá»§a má»—i ngÆ°á»i chÆ¡i
mutable struct FictitiousPlay
    ğ’« # simple game
    i # agent index
    N # array of action count dictionaries
    Ï€i # current policy
end

# hÃ m táº¡o má»™t FictitiousPlay
function FictitiousPlay(ğ’«::SimpleGame, i)
    N = [Dict(aj => 1 for aj in ğ’«.ğ’œ[j]) for j in ğ’«.â„]
    Ï€i = SimpleGamePolicy(ai => 1.0 for ai in ğ’«.ğ’œ[i])
    return FictitiousPlay(ğ’«, i, N, Ï€i)
end

(Ï€i::FictitiousPlay)() = Ï€i.Ï€i()
(Ï€i::FictitiousPlay)(ai) = Ï€i.Ï€i(ai)

# cáº­p nháº­t cÃ¡ch chÆ¡i Ï€i vá»›i sá»± lá»±a chá»n a
function update!(Ï€i::FictitiousPlay, a)
    N, ğ’«, â„, i = Ï€i.N, Ï€i.ğ’«, Ï€i.ğ’«.â„, Ï€i.i
    for (j, aj) in enumerate(a)
        N[j][aj] += 1
    end
    p(j) = SimpleGamePolicy(aj => u/sum(values(N[j])) for (aj, u) in N[j])
    Ï€ = [p(j) for j in â„]
    Ï€i.Ï€i = best_response(ğ’«, Ï€, i)
end
# [1]

# lÆ°u sá»± lá»±a chá»n vÃ  Ä‘iá»ƒm sá»‘ cá»§a tá»«ng ngÆ°á»i chÆ¡i 
function write_score!(Ï€, score, agent, policy)
    Ï€1, Ï€2 = Ï€
    p = [collect(keys(Ï€1.Ï€i.p))[1], collect(keys(Ï€2.Ï€i.p))[1]]
    res = joint_reward(RockPaperScissors(), p)
    score[1] += res[1]
    score[2] += res[2]
    push!(agent[1], score[1])
    push!(agent[2], score[2])
    for i = 1:2
        if p[i] == :rock
            push!(policy[i][1], 1)
            push!(policy[i][2], 0)
            push!(policy[i][3], 0)
        elseif p[i] == :paper
            push!(policy[i][1], 0)
            push!(policy[i][2], 1)
            push!(policy[i][3], 0)
        elseif p[i] == :scissors
            push!(policy[i][1], 0)
            push!(policy[i][2], 0)
            push!(policy[i][3], 1)
        end
    end
end

# lÆ°u phÃ¢n phá»‘i xÃ¡c suáº¥t cÃ¡c hÃ nh Ä‘á»™ng cá»§a tá»«ng ngÆ°á»i chÆ¡i
function write_prop!(Ï€, prop)
    Ï€1, Ï€2 = Ï€
    for i = 1:2
        push!(prop[i][1], (Ï€1.N[i][:rock] / sum(values(Ï€1.N[i]))))
        push!(prop[i][2], (Ï€1.N[i][:paper] / sum(values(Ï€1.N[i]))))
        push!(prop[i][3], (Ï€1.N[i][:scissors] / sum(values(Ï€1.N[i]))))
    end
end
# váº½ Ä‘á»“ thá»‹ 
function visualization(k_max, agent, prop, policy)
    # váº½ Ä‘á»“ thá»‹ scores of two agents
    plot(1:k_max, agent[1], title = "Scores of two agents", label = "Agent 1")
    pScore = plot!(1:k_max, agent[2], label = "Agent 2")
    xlabel!("Iteration")
    savefig(pScore, joinpath(@__DIR__, "score.png"))
    # váº½ Ä‘á»“ thá»‹ Opponent model of Agent 2
    plot(1:k_max, prop[1][1], title = "Opponent model of Agent 2", label = "Rock")
    pProp1 = plot!(1:k_max, prop[1][2], label = "Paper")
    pProp1 = plot!(1:k_max, prop[1][3], label="Scissors")
    xlabel!("Iteration")
    savefig(pProp1, joinpath(@__DIR__, "opponent_model2.png"))
    # váº½ Ä‘á»“ thá»‹ Opponent model of Agent 1
    plot(1:k_max, prop[2][1], title = "Opponent model of Agent 1", label = "Rock")
    pProp2 = plot!(1:k_max, prop[2][2], label="Paper")
    pProp2 = plot!(1:k_max, prop[2][3], label="Scissors")
    xlabel!("Iteration")
    savefig(pProp2, joinpath(@__DIR__, "opponent_model1.png"))
    # váº½ Ä‘á»“ thá»‹ Policy of Agent 1
    plot(1:k_max, policy[1][1], title = "Policy of Agent 1", label = "Rock")
    pPolicy1 = plot!(1:k_max, policy[1][2], label="Paper")
    pPolicy1 = plot!(1:k_max, policy[1][3], label="Scissors")
    xlabel!("Iteration")
    savefig(pPolicy1, joinpath(@__DIR__, "policy1.png"))
    # váº½ Ä‘á»“ thá»‹ Policy of Agent 2
    plot(1:k_max, policy[2][1], title = "Policy of Agent 2", label = "Rock")
    pPolicy2 = plot!(1:k_max, policy[2][2], label="Paper")
    pPolicy2 = plot!(1:k_max, policy[2][3], label="Scissors")
    xlabel!("Iteration")
    savefig(pPolicy2, joinpath(@__DIR__, "policy2.png"))
end

# [1]
# mÃ´ phá»ng trÃ² chÆ¡i RPS ğ’« vá»›i Ï€ lÃ  chiáº¿n lÆ°á»£c chÆ¡i vÃ  k_max lÃ  sá»‘ lÆ°á»£t chÆ¡i
function simulate(ğ’«::SimpleGame, Ï€, k_max)
    # máº£ng chá»©a sá»‘ Ä‘iá»ƒm cá»§a 2 ngÆ°á»i chÆ¡i
    score = [0, 0] 
    # máº£ng chá»©a sá»‘ Ä‘iá»ƒm cá»§a 2 ngÆ°á»i chÆ¡i theo cÃ¡c láº§n chÆ¡i 
    agent = [[], []]
    # máº£ng chá»©a xÃ¡c suáº¥t cÃ¡c hÃ nh Ä‘á»™ng Ä‘Ã£ chá»n cá»§a 2 ngÆ°á»i chÆ¡i theo cÃ¡c láº§n chÆ¡i 
    prop = [[[], [], []], [[], [], []]]
    # máº£ng chá»©a cÃ¡c lá»±a chá»n cá»§a 2 ngÆ°á»i chÆ¡i theo cÃ¡c láº§n chÆ¡i
    policy = [[[], [], []], [[], [], []]]
    # duyá»‡t k_max vÃ²ng
    for k = 1:k_max
        # a lÃ  máº£ng sá»± lá»±a chá»n cá»§a 2 ngÆ°á»i chÆ¡i
        a = [Ï€i() for Ï€i in Ï€]
        # cáº­p nháº­t chiáº¿n lÆ°á»£c cá»§a tá»«ng ngÆ°á»i chÆ¡i
        for Ï€i in Ï€
            update!(Ï€i, a)
        end
        # lÆ°u káº¿t quáº£ vÃ o agent, prop, policy Ä‘á»ƒ váº½ visualization
        write_score!(Ï€, score, agent, policy)
        write_prop!(Ï€, prop)
    end
    # váº½ visualization
    visualization(k_max, agent, prop, policy)
    return Ï€
end
# [1]
# gá»i simulate Ä‘á»ƒ thá»±c hiá»‡n trÃ² chÆ¡i
function main()
    ğ’« = SimpleGame(RockPaperScissors())
    Ï€1 = FictitiousPlay(ğ’«, 1)
    Ï€2 = FictitiousPlay(ğ’«, 2)
    Ï€ = [Ï€1, Ï€2]
    simulate(ğ’«, Ï€, 10000)
end
main()