#CODE REFER
#Algorithms for Decision Making
#https://github.com/algorithmsbooks/DecisionMakingProblems.jl

#Import Library
using JuMP,  Ipopt
using Distributions
using Random
using LinearAlgebra, BenchmarkTools
using DecisionMakingProblems

#define constant
SING = "SING"
CRYING = "CRYING"
QUIET = "QUIET"
FEED = "FEED"
SATED = "SATED"
HUNGRY = "HUNGRY"
##########################################################

#struct define 
struct POMG
    Î³  # discount factor
    â„  # agents
    ğ’®  # state space
    ğ’œ  # joint action space
    ğ’ª  # joint observation space
    T  # transition function
    O  # joint observation function
    R  # joint reward function

    function POMG(discount, agents, states, jointAction, jointObservation, transitionFunction, jointObservationFunction, jointReward)
        new(discount, agents, states, jointAction, jointObservation, transitionFunction, jointObservationFunction, jointReward);
    end
end

struct POMGDynamicProgramming
    b # initial belief
    d # depth of conditional plans
end

struct ConditionalPlan
    a # action to take at root
    subplans # dictionary mapping observations to subplans
end

ConditionalPlan(a) = ConditionalPlan(a, Dict())

(Ï€::ConditionalPlan)() = Ï€.a
(Ï€::ConditionalPlan)(o) = Ï€.subplans[o]

function expand_conditional_plans(ğ’«, Î )
    â„, ğ’œ, ğ’ª = ğ’«.â„, ğ’«.ğ’œ, ğ’«.ğ’ª
    return [[ConditionalPlan(ai, Dict(oi => Ï€i for oi in ğ’ª[i]))
        for Ï€i in Î [i] for ai in ğ’œ[i]] for i in â„]
end

joint(X) = vec(collect(Iterators.product(X...)))
joint(Ï€, Ï€i, i) = [i == j ? Ï€i : Ï€j for (j, Ï€j) in enumerate(Ï€)]

struct SimpleGame
    Î³  # discount factor
    â„  # agents
    ğ’œ  # joint action space
    R  # joint reward function
end

struct NashEquilibrium end

struct SimpleGamePolicy
    p # dictionary mapping actions to probabilities
    function SimpleGamePolicy(p::Base.Generator)
        return SimpleGamePolicy(Dict(p))
    end

    function SimpleGamePolicy(p::Dict)
        vs = collect(values(p))
        vs ./= sum(vs)
        return new(Dict(k => v for (k,v) in zip(keys(p), vs)))
    end

    SimpleGamePolicy(ai) = new(Dict(ai => 1.0))
end
##########################################################

#define transition function
function transition(s, a, sâ€²)
    # Regardless, feeding makes the baby sated.
    if a[1] == "FEED" || a[2] == "FEED"
        if sâ€² == "SATED"
            return 1.0
        else
            return 0.0
        end
    else
        # If neither caretaker fed, then one of two things happens.
        # First, a baby that is hungry remains hungry.
        if s == "HUNGRY"
            if sâ€² == "HUNGRY"
                return 1.0
            else
                return 0.0
            end
        # Otherwise, it becomes hungry with a fixed probability.
        else
            probBecomeHungry = 0.5 #pomg.babyPOMDP.p_become_hungry
            if sâ€² == "SATED"
                return 1.0 - probBecomeHungry
            else
                return probBecomeHungry
            end
        end
    end
end

#JOINT OBSERVERTAION FUNCTION FOR POMG STRUCT
function joint_observation(a, sâ€², o)
    # If at least one caregiver sings, then both observe the result.
    p_cry_when_hungry_in_sing = 0.9
    p_cry_when_hungry = 0.9
    p_cry_when_not_hungry = 0.0

    if a[1] == SING || a[2] == SING
        # If the baby is hungry, then the caregivers both observe crying/silent together.
        if sâ€² == HUNGRY
            if o[1] == CRYING && o[2] == CRYING
                return p_cry_when_hungry_in_sing
            elseif o[1] == QUIET && o[2] == QUIET
                return 1.0 - p_cry_when_hungry_in_sing
            else
                return 0.0
            end
        # Otherwise the baby is sated, and the baby is silent.
        else
            if o[1] == QUIET && o[2] == QUIET
                return 1.0
            else
                return 0.0
            end
        end
    # Otherwise, the caregivers fed and/or ignored the baby.
    else
        # If the baby is hungry, then there's a probability it cries.
        if sâ€² == HUNGRY
            if o[1] == CRYING && o[2] == CRYING
                return p_cry_when_hungry
            elseif o[1] == QUIET && o[2] == QUIET
                return 1.0 - p_cry_when_hungry
            else
                return 0.0
            end
        # Similarly when it is sated.
        else
            if o[1] == CRYING && o[2] == CRYING
                return p_cry_when_not_hungry
            elseif o[1] == QUIET && o[2] == QUIET
                return 1.0 - p_cry_when_not_hungry
            else
                return 0.0
            end
        end
    end
end

#REWARD FUNCTION FOR POMG STRUCT
function joint_reward(s, a)
    r = [0.0, 0.0]

    # Both caregivers do not want the child to be hungry.
    if s == HUNGRY
        r -= [10, 10]
    end

    # One caregiver prefers to feed.
    if a[1] == FEED
        r[1] -= 2.5
    elseif a[1] == SING
        r[1] -= 0.5
    end

    # One caregiver prefers to sing.
    if a[2] == FEED
        r[2] -= 5
    elseif a[2] == SING
        r[2] -= 0.25
    end

    # Note that caregivers only experience a cost if they do something.
    return r
end


function lookahead(ğ’«::POMG, U, s, a)
    ğ’®, ğ’ª, T, O, R, Î³ = ğ’«.ğ’®, joint(ğ’«.ğ’ª), ğ’«.T, ğ’«.O, ğ’«.R, ğ’«.Î³
    uâ€² = sum(T(s,a,sâ€²)*sum(O(a,sâ€²,o)*U(o,sâ€²) for o in ğ’ª) for sâ€² in ğ’®)
    return R(s,a) + Î³*uâ€²
end

function evaluate_plan(ğ’«::POMG, Ï€, s)
    a = Tuple(Ï€i() for Ï€i in Ï€)
    U(o,sâ€²) = evaluate_plan(ğ’«, [Ï€i(oi) for (Ï€i, oi) in zip(Ï€,o)], sâ€²)
    return isempty(first(Ï€).subplans) ? ğ’«.R(s,a) : lookahead(ğ’«, U, s, a)
end

#Kiem tra xem nhanh i co bi thong tri boi nhanh nao khac hay khong
function is_dominated(ğ’«::POMG, Î , i, Ï€i)
    â„, ğ’® = ğ’«.â„, ğ’«.ğ’®
    jointÎ noti = joint([Î [j] for j in â„ if j â‰  i])
    Ï€(Ï€iâ€², Ï€noti) = [j==i ? Ï€iâ€² : Ï€noti[j>i ? j-1 : j] for j in â„]
    Ui = Dict((Ï€iâ€², Ï€noti, s) => evaluate_plan(ğ’«, Ï€(Ï€iâ€², Ï€noti), s)[i]
            for Ï€iâ€² in Î [i], Ï€noti in jointÎ noti, s in ğ’®)
    model = Model(Ipopt.Optimizer)
    @variable(model, Î´)
    @variable(model, b[jointÎ noti, ğ’®] â‰¥ 0)
    @objective(model, Max, Î´)
    @constraint(model, [Ï€iâ€²=Î [i]],
        sum(b[Ï€noti, s] * (Ui[Ï€iâ€², Ï€noti, s] - Ui[Ï€i, Ï€noti, s])
        for Ï€noti in jointÎ noti for s in ğ’®) â‰¥ Î´)
    @constraint(model, sum(b) == 1)
    optimize!(model)
    return value(Î´) â‰¥ 0
end

#Cat bo cac nhanh policy bi "thong tri" boi mot nhanh khac
function prune_dominated!(Î , ğ’«::POMG)
    done = false
    while !done
        done = true
        for i in shuffle(ğ’«.â„)
            for Ï€i in shuffle(Î [i])
                if length(Î [i]) > 1 && is_dominated(ğ’«, Î , i, Ï€i)
                    filter!(Ï€iâ€² -> Ï€iâ€² â‰  Ï€i, Î [i])
                    done = false
                    break
                end
            end
        end
    end
end

function tensorform(ğ’«::SimpleGame)
    â„, ğ’œ, R = ğ’«.â„, ğ’«.ğ’œ, ğ’«.R
    â„â€² = eachindex(â„)
    ğ’œâ€² = [eachindex(ğ’œ[i]) for i in â„]
    Râ€² = [R(a) for a in joint(ğ’œ)]
    return â„â€², ğ’œâ€², Râ€²
end

function solve(M::NashEquilibrium, ğ’«::SimpleGame)
    â„, ğ’œ, R = tensorform(ğ’«)
    model = Model(Ipopt.Optimizer)
    @variable(model, U[â„])
    @variable(model, Ï€[i=â„, ğ’œ[i]] â‰¥ 0)
    @NLobjective(model, Min,
        sum(U[i] - sum(prod(Ï€[j,a[j]] for j in â„) * R[y][i]
            for (y,a) in enumerate(joint(ğ’œ))) for i in â„))
    @NLconstraint(model, [i=â„, ai=ğ’œ[i]],
        U[i] â‰¥ sum(
            prod(j==i ? (a[j]==ai ? 1.0 : 0.0) : Ï€[j,a[j]] for j in â„)
            * R[y][i] for (y,a) in enumerate(joint(ğ’œ))))
    @constraint(model, [i=â„], sum(Ï€[i,ai] for ai in ğ’œ[i]) == 1)
    optimize!(model)
    Ï€iâ€²(i) = SimpleGamePolicy(ğ’«.ğ’œ[i][ai] => value(Ï€[i,ai]) for ai in ğ’œ[i])
    return [Ï€iâ€²(i) for i in â„]
end

function utility(ğ’«::POMG, b, Ï€)
    u = [evaluate_plan(ğ’«, Ï€, s) for s in ğ’«.ğ’®]
    return sum(bs * us for (bs, us) in zip(b, u))
end

#Ham giai bai toan POMG bang thuat toan Dynamic programming
function solve(M::POMGDynamicProgramming, ğ’«::POMG)
    â„, ğ’®, ğ’œ, R, Î³, b, d = ğ’«.â„, ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.R, ğ’«.Î³, M.b, M.d
    Î  = [[ConditionalPlan(ai) for ai in ğ’œ[i]] for i in â„]
    for t in 1:d
        Î  = expand_conditional_plans(ğ’«, Î )
        prune_dominated!(Î , ğ’«)
    end
    ğ’¢ = SimpleGame(Î³, â„, Î , Ï€ -> utility(ğ’«, b, Ï€))
    Ï€ = solve(NashEquilibrium(), ğ’¢)
    return Tuple(argmax(Ï€i.p) for Ï€i in Ï€)
end

#Declare problem base on POMG struct
multicare = POMG(0.9, 
                [1, 2], 
                ["HUNGRY", "SATED"], 
                [["FEED", "SING", "IGNORE"], ["FEED", "SING", "IGNORE"]], 
                [["CRYING", "QUIET"], ["CRYING", "QUIET"]], 
                transition, 
                joint_observation, 
                joint_reward);

b = [0.5, 0.5];
dyP = POMGDynamicProgramming(b, 1);

result = solve(dyP, multicare);
print(result)